import uvicorn
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from verifier import verify_ai_output
import requests

# Lazy-import heavy modules inside endpoints to keep startup light

app = FastAPI(title="Website-to-RAG Chatbot")

# --- CORS for extension/frontend (dev-friendly) ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Allow Private Network Access preflight for localhost in Chrome
@app.middleware("http")
async def add_private_network_header(request, call_next):
    response = await call_next(request)
    if request.method == "OPTIONS":
        response.headers["Access-Control-Allow-Private-Network"] = "true"
    return response

# --- Pydantic Models ---
class IngestRequest(BaseModel):
    url: str

class YouTubeIngestRequest(BaseModel):
    video_url: str
    site_id: str | None = None

class ChatRequest(BaseModel):
    question: str
    site_id: str | None = None
    mode: str = "rag"  # "rag" or "general"
    source_types: list[str] | None = None  # ["website", "youtube", "pdf"]

# --- Endpoints ---
@app.post("/ingest")
def ingest_site(req: IngestRequest):
    from crawler import crawl_site, clean, get_site_id
    from chunker import chunk_text
    from embeddings import embed_chunks
    from store import store_chunks
    from site_checker import site_exists
    from translator import translate_to_english
    try:
        from langdetect import detect, LangDetectException
    except ImportError:
        detect = None
        LangDetectException = Exception

    site_id = get_site_id(req.url)

    # Check if site has already been ingested
    existing = site_exists(site_id)
    if existing:
        return {
            "status": "success",
            "chunks_indexed": existing["chunks_indexed"],
            "pages_crawled": existing["chunks_indexed"],  # Approximate; not tracked separately
            "site_id": site_id,
            "url": req.url,
            "cached": True,  # Indicate data was already in DB
        }

    # Use deeper crawl to capture contact, history, about pages
    pages = crawl_site(req.url, max_depth=3, max_pages=40)

    total_chunks = 0
    for page_url, html in pages:
        text = clean(html)
        # Skip only extremely sparse pages; keep concise but meaningful content
        if len(text.strip()) < 100:
            continue
        
        # Detect language and translate if needed
        language = "en"
        content_original = None
        if detect:
            try:
                language = detect(text)
                if language != "en":
                    content_original = text
                    text = translate_to_english(text, language)
            except LangDetectException:
                # If detection fails, assume English
                language = "en"
        
        chunks = chunk_text(text)
        if not chunks:
            continue
        embeddings = embed_chunks(chunks)
        # Store with the specific page URL, site_id, and language metadata
        store_chunks(chunks, embeddings, page_url, site_id, language=language, content_original=content_original)
        total_chunks += len(chunks)

    return {
        "status": "success",
        "chunks_indexed": total_chunks,
        "pages_crawled": len(pages),
        "site_id": site_id,
        "url": req.url,
        "cached": False,
    }

@app.post("/chat")
def chat(req: ChatRequest):
    # Both modes use RAG retrieval for context; mode controls answer behavior
    if not req.site_id:
        raise HTTPException(status_code=400, detail="site_id is required")
    from rag import rag_chat
    result = rag_chat(req.question, site_id=req.site_id, source_types=req.source_types, mode=req.mode)
    result["mode"] = req.mode
    return result

@app.post("/ingest/youtube")
def ingest_youtube(req: YouTubeIngestRequest):
    from youtube_ingest import ingest_youtube_video
    from chunker import chunk_text
    from embeddings import embed_chunks
    from store import store_chunks
    from translator import translate_to_english
    try:
        from langdetect import detect, LangDetectException
    except ImportError:
        detect = None
        LangDetectException = Exception

    try:
        # Ingest YouTube video transcript
        result = ingest_youtube_video(req.video_url, req.site_id)
        final_site_id = req.site_id or f"youtube-{result['video_id']}"
        
        # Extract text from chunks for further processing
        texts = [chunk['text'] for chunk in result['chunks']]
        
        # Chunk the transcript text (combine segments for better context)
        combined_text = " ".join(texts)
        
        # Detect language and translate once per video if needed
        language = "en"
        content_original = None
        if detect:
            try:
                language = detect(combined_text)
                if language != "en":
                    content_original = combined_text
                    combined_text = translate_to_english(combined_text, language)
            except LangDetectException:
                language = "en"
        
        chunks = chunk_text(combined_text)
        
        # Generate embeddings
        embeddings = embed_chunks(chunks)
        
        # Store with metadata
        for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            # Estimate timestamp based on position in text
            segment_index = min(i * len(result['chunks']) // len(chunks), len(result['chunks']) - 1)
            timestamp = result['chunks'][segment_index]['metadata']['timestamp']
            
            store_chunks(
                [chunk], 
                [emb], 
                req.video_url, 
                final_site_id,
                source_type="youtube",
                timestamp=timestamp,
                language=language,
                content_original=content_original
            )
        
        return {
            "status": "success",
            "chunks_indexed": len(chunks),
            "segments_processed": result['segments_count'],
            "site_id": final_site_id,
            "video_id": result['video_id'],
            "source_url": result['source_url']
        }
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest YouTube video: {str(e)}")


@app.post("/ingest/pdf")
async def ingest_pdf(
    request: Request,
    file: UploadFile | None = File(None),
    url: str | None = Form(None),
    site_id: str | None = Form(None),
):
    from pdf_ingest import ingest_pdf_document
    from chunker import chunk_text
    from embeddings import embed_chunks
    from store import store_chunks
    from translator import translate_to_english
    try:
        from langdetect import detect, LangDetectException
    except ImportError:
        detect = None
        LangDetectException = Exception

    # Support both multipart/form-data (file or url) and application/json (url)
    if file is None and not url:
        try:
            body = await request.json()
            url = body.get("url")
            site_id = body.get("site_id") or site_id
        except Exception:
            pass

    if file is None and (not url or not url.strip()):
        raise HTTPException(status_code=400, detail="Provide either a PDF file or a PDF URL")

    # Get PDF bytes and source URL label
    if file is not None:
        file_bytes = await file.read()
        source_url = file.filename or "uploaded.pdf"
    else:
        try:
            resp = requests.get(url, timeout=15)
            resp.raise_for_status()
            file_bytes = resp.content
            source_url = url
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Failed to fetch PDF: {str(e)}")

    try:
        doc = ingest_pdf_document(file_bytes, source_url, site_id)
        final_site_id = doc["site_id"]

        # Chunk per page to preserve page_number metadata
        chunk_texts = []
        page_numbers = []
        languages = []
        content_originals = []
        
        for page in doc["pages"]:
            page_text = page["text"]
            
            # Detect language and translate per page
            language = "en"
            content_original = None
            if detect:
                try:
                    language = detect(page_text)
                    if language != "en":
                        content_original = page_text
                        page_text = translate_to_english(page_text, language)
                except LangDetectException:
                    language = "en"
            
            pieces = chunk_text(page_text)
            for p in pieces:
                chunk_texts.append(p)
                page_numbers.append(page["page_number"])
                languages.append(language)
                content_originals.append(content_original)

        if not chunk_texts:
            raise ValueError("No text extracted from PDF")

        embeddings = embed_chunks(chunk_texts)

        for chunk, emb, page_num, language, content_original in zip(
            chunk_texts, embeddings, page_numbers, languages, content_originals
        ):
            store_chunks(
                [chunk],
                [emb],
                source_url,
                final_site_id,
                source_type="pdf",
                page_number=page_num,
                language=language,
                content_original=content_original            )

        return {
            "status": "success",
            "chunks_indexed": len(chunk_texts),
            "pages_processed": doc["pages_count"],
            "site_id": final_site_id,
            "source_url": source_url,
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest PDF: {str(e)}")

@app.post("/recrawl")
def recrawl(req: IngestRequest):
    # For hackathon: same as ingest (overwrite logic handles duplicates typically)
    return ingest_site(req)

@app.post("/verify-ai-output")
async def verify_ai(payload: dict):
    """
    Verifies hallucination in AI-generated output.
    Expected payload:
    {
        "prompt": "...",
        "response": "..."
    }
    """
    prompt = payload.get("prompt", "")
    response = payload.get("response", "")

    if not response:
        return {
            "error": "AI response missing"
        }

    result = verify_ai_output(prompt, response)
    return result


# --- Startup Logic ---
if __name__ == "__main__":
    # This block only runs if you execute: python main.py
    uvicorn.run(
        "main:app", 
        host="127.0.0.1", 
        port=8000, 
        reload=True  # Automatically restarts when you make code changes
    )
